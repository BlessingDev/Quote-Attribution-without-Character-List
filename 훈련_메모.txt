
다음 실험에서는 anchor 수를 줄이고, lr을 올려보자

HDBSCAN 대신 Affinity Propagation을 해보자(나중)

정말 anchor 수와 memory step이 성능에 영향을 주는지 확인해 볼 필요가 있을 것 같다.

군집이 너무 안 만들어지는 경향이 있다.

loss에 대한 분석도 필요하다.
분석 완료. neg_spcl의 영향을 줄일 필요가 있음.

우리 task는 발화자를 식별하는 것이기에 굳이 형태도 다르고 담는 내용도 천차만별일 narrative를 하나의 군집으로 모이게 할 필요는 없을 것이다.
이들이 오히려 훈련에 지대한 악영향을 끼치고 있을 수 있다.

세팅 변경 이력
0529_1724부터
- log(l+1) 로스 적용

0530_1245
- 마지막 선형 변환 후에 tanh 변환 적용
- 의미 없음. 오히려 약간 떨어짐.

0530_1814
- tanh층 제거
- mem step 늘림
- 확실히 더 일반화되는 느낌 있음. v1과 loss 사이의 consistency도 증가.

0531_0955
- anchor 저장 무한하게
- 별로 다른 모습을 보여주지 못함

0531_1341
- narrative를 군집화에서 제외(positive loss 계산 안 함)
- loss에 beta 도입(기본값 0.5 사용)
- loss가 낮은 수치로 나오긴 했으나 V1이 거의 그대로 나옴. narrative를 안 세어서 얻는 메리트 없는 것으로 보임. 

0531_1637
- beta에서 centroid에 더 가중치를 두도록 한다.
- anchor 계산시 narrative는 제외
- 변경한 설정의 영향이 너무 작았다. lr을 5배 올렸다.
- NAN weight 발생

0601_1119
- anchor 무한 상태에서 mem step 늘려보기
- beta에서 centroid에 더 가중치를 두도록 한다.
- 별로 효과 없음

0603_1017
- loss 재설계
    - 편차가 커지면 phi가 작아져 해당 centroid를 더 고려하도록 phi 계산식을 수정
    - 거리가 가까운 negative point, 먼 positive point를 더 고려하도록 한다. 단순 평군으로는 거리가 먼 negative와 가까운 negative가 있으면 평균은 그 사이 값으로 적절하게 나올 것이다. 다수의 포인트를 가까운 곳에 모아야 하는 문제 특성상 이는 적절하지 않다.
    - 가까운 걸 멀리 보내려면 exp, 먼 걸 가까이 가져오려면 log. 현재는 활성 함수가 무조건 exp이니 가까운 걸 멀리 보내는 데만 유리하다.
    - 확률 개념이 아니니 temperature를 적용해야 할 직관적인 이유가 없다. phi만 적용.
- numerical 불안정 문제 발생
- 성능은 다소 향상되는 것으로 확인

0603_1326
- 수치 불안정성 문제 해결을 위해 weight decay, gradient clipping 추가

0603_1820
- loss에 alpha beta 추가
- 성능이 상당히 향상되는 모습을 보여줬지만, weight decay 없이는 수치 불안정성 발생

0604_1058
- max_grad_norm 축소
- MLP에 은닉층 추가
- weight decay 0.1
- 별로 인상적이지 않은 성능

0604_1335
- MLP 은닉층 한 개 제거
- loss를 gradient 관점에서 재설계
    - negative에 log를 적용하여 가까이 있는 점을 빠르게 밀어낸다.
    - postive에 exp를 적용하여 멀리 있는 점을 빠르게 끌고온다.
- loss_sig를 centroid를 중시하도록 설정
- 별로 효과적이지 않았음
- 후속 조치로 실제 거리가 어느 정도 선에서 나오는지 확인해볼 계획

0604_1817
- loss를 재설계
    - negative set을 log에서 선형으로 변경. 일정 이상 멀어지면 gradient가 급격히 소멸하는 부분이 문제라고 생각
    - alpha를 1로 변경, beta 0.8
- 그다지 인상적이지 않은 성능

0605_1111
- loss를 재설계
    - negative set을 선형에서 n*log(n+e) 형태로 변형. 초반에 빠르게 멀어지고 그 이후에도 어느정도 gradient를 유지하게 될 것
- 한 지점에 모든 점이 수렴하지 않고, negative 간에 멀어지면서 점점 퍼지는 모습을 확인함
- loss가 일정 이하로 내려가지 못하고 횡보하는 양상 보임. 언더피팅
- 다음 실험에서는 모델 복잡도를 높여서 시도

0605_1652
- 모델 복잡도 올림. 
    - MLP layer 추가. 
    - Add&Norm 층 추가.
- loss 재설계
    - loss를 살펴보았을 때 batch내 pos간 거리는 비교적 짧게 나오고, centroid 거리는 비교적 멀게 나온다.
    - batch내 negative간 거리는 짧게 나오고 centroid와의 거리가 길게 나온다.
    - 따라서 sig 가중치를 교차해서 적용하도록 재설계했다.
    - pos에서는 centroid와의 거리를 더 고려해야 하고, negative에서는 batch내 샘플 간 거리를 더 고려해야 한다.
- phi 계산식 수정
- homogenity가 유래 없이 증가하는 양상을 확인. 이는 노린 바에 부합
- 수치 불안정성이 부각됨. postive 거리가 멀어질수록 어느 순간 exp 함수가 inf로 발산할 위험이 생김.

0605_1800
- loss 재설계
    - 마이너 변경
    - alpha를 더 작은 수로 변경. beta도 비슷한 비율로 변경. (0.15, 0.2)
    - exp은 발산하는 함수이기 때문에 거리가 먼 점을 될 수 있으면 가까이 끌어와야 함
    - homogeniety를 높인다는 성과를 달성했으므로 컨셉은 그대로 가져감
- 상당히 높은 성능을 보임
- 그러나 수치 불안정성이 너무 눈에 띔

0606_1747
- loss 값을 모두 더하여 나중에 log를 취하는 것이 아니라 각 샘플마다 log를 구해서 더하도록 코딩 변경
- 실제 로스 값 계산을 확인해보니 좌표 수치가 16~-16까지 벌어지는 것을 확인.
    - normalize한 거리도 수백 수준까지 증가, 그 결과 loss가 inf로 발산한 듯함.
    - 너무 높은 자유도. 이를 해결하기 위해 최종 층에 활성 함수로 tanh를 추가함
- 결과적으로 loss가 0으로 수렴했지만, 군집화 성능은 좋지 않았다.
    - 이는 loss 구조의 태생적인 문제로부터 기인한 것으로 보인다. 
    - pos만 0으로 만들면 negative가 얼마나 되는지와 관계 없이 loss가 0이 된다. 이는 clustering의 취지와도 어긋난다.

0606_2118
- 모델 저장 기준을 v1 점수로 변경
- mu_p, mu_neg을 0.5, 0.2로 변경, negative를 더 강하게 밀어내도록 했다.
- pos / neg 계산을 (pos + 1) / neg로 변경하여 pos를 0으로 만든 후에도 neg를 계속 밀어내도록 하였다.
- tanh를 적용한 상태에서 이전과 유사한 성능을 달성. 그러나 0605_1800에는 못 미치는 성능 보임
- loss 최적화가 잘 되지 않는 모습을 보이며 언더피팅

0607_0931
- 언더피팅의 이유가 자유도 부족에 있다고 가정. decoder 차원을 2048에서 5000으로 늘림
- 이전에 잘 되었던 mu_p와 mu_n인 0.15와 0.2와 비슷하게 0.5, 0.6으로 재설정
- 성능 잘 안 나옴. 오히려 0606_2118보다 못한 모습 보임
- 자유도를 늘렸음에도 언더피팅. loss 최적화가 왜 안 되는지 이유 분석 필요

0607_1535
- 멀리 있는 postive를 빠르게 끌어오도록 mu_p를 크게, 가까이 있는 negative를 더 열심히 밀어내도록 mu_n을 작게 설정 (0.8, 0.3)
- loss가 줄어들지 않는 현상을 고치기 위해 lr 증가
- 최적화를 위해 단일 책 최적화를 n회씩 진행하는 기법 (n에폭씩 묶어서 에폭을 진행)
- 성능 여전히 안 좋음. 최적화 안 됨.

0607_1754
- lr 더 증가
- mu_p와 mu_n을 이전 가장 좋았던 값인 0.15와 0.2로 변경
    - tanh에 자유도를 추가. 5로 설정
- 성능 너무 나쁨

0608_1214
- mse_loss의 reduction을 기존 sum에서 mean으로 변경.
    - 그에 따라 수의 규모가 줄어드니 mu_p를 1.0으로 변경
    - mu_n도 0.8로 변경
    - tanh 활성 제거
- 성능이 매우 나쁨
- 단순 성능 비교를 위해 여기서부터 파라미터를 하나씩 바꾸며 실험 진행

0608_2203
- 위 설정에서 mu_p가 미치는 영향을 파악하기 위해 mu_p만 0.5로 줄임
- 안 좋은 성능


0610_1138
- 위 설정에서 mu_n이 미치는 영향을 파악하기 위해 mu_p를 1.0으로 mu_n을 0.5로 설정
- tanh 활성 추가
- 첫 에폭에서 발화자 비율 파이 차트 그리기
- 매 반복마다 평균 pos 거리, 평균 neg 거리 기록. 도표 그리기.
- 군집화 평가를 Fowlkes-Mallows Score로 진행

0701_
- mse_epsilon 실험 시작
    - loss 재설계(epsilon을 두어 positive와 negative가 epsilon 이상의 차이를 가지도록 loss 설정)
    - 디코더에 gru 추가함

0702_1453
- negative 거리와 positive 거리고 동등하게 고려되어야 epsilon 항에 의미가 있음
    - x*log(x) 식은 negative 거리를 뻥튀기해주는 효과가 존재(e.x. 동일하게 거리가 4점 대인데 x*log(x)를 거치는 negative 거리는 8로 뻥튀기되어 더 먼 것처럼 됨)
    - negative 거리도 선형으로 변경
- 기존 mse는 거리를 측정하는 기법으로 부적절. cdist 함수를 사용하여 l2 거리를 직접 측정하도록 함
- positive 샘플 수가 0개일 때 정규화 과정에서 0 나누기로 인해 nan이 생기는 오류를 수정
- 결과
    - 많이 등장하는 캐릭터의 거리를 확 늘리는 것으로 다른 캐릭터에서 거리가 잘 안 나오는 것을 벌충하려고 들음
    - 현재 손실 함수는 positive 거리가 마구 커져도 negative 거리가 비슷한 비율로 커지면 문제가 없음
- 해법
    - 화자 비율을 계산에 반영.
    - postive 거리의 절댓값으로 penalty

0702_1622
- loss 재설계
    - 화자 비율 반영
    - postive 거리의 제곱으로 penalty

0704_
- 모델의 최종 head를 MLP로 변경

0705
- FM 점수가 잘 작동하지 않음을 발견
    - adjusted random index로 metric을 변경
- postive 거리 제곱으로 penalty를 주는 것이 다소 약한 듯하여 exp로 변경
- 인명을 식별할 때 대문자 여부가 중요. bert 모델을 cased로 변경

0708
- BERT를 RoBERTa로 변경
- k-means로 anchor를 찾을 때 평균점과 anchor 거리가 일정 이상 떨어져 있으면 anchor를 할당하지 않음.
    - 주로 소수 label에 anchor가 할당되지 않을 가능성이 높음
- loss 수정
    - 설명문은 아예 loss 계산에서 빼고 발화문 간에만 clustering loss 계산
    - 대신에 별개의 head를 두어 설명문인가 여부를 분류하도록 개선
    - 우선 별개의 파라미터 없이 두 loss를 동일하게 더해서 사용
    - loss 계산시 narrative 분류 정확도만큼 contrastive loss를 통합
- clustering 기법을 hdbscan에서 bisecting k-means로 변경
- 결과
    - classification을 거니 공간을 분할하는 능력도 함께 향상되는 양상을 보임. contrastive loss를 사용하는 것보다 분류기를 두는 것이 더 유용할지도 모름
    - 차원은 (1024, 2048)로 실험 범위에서 가장 작을 때, 그리고 앵커를 계산하지 않을 때(sigma=1.0) 혹은 앵커만 계산할 때(sigma=0.0) 모델이 비교적 높은 성능을 보였다.
    - 최고 성능은 차원 (1024, 2048), sigma=1.0에서 달성했다.

0731
- 각 책 별로 화자 분류 task를 수행하도록 훈련. 우선 각 책 단위에서 모델이 분류 task를 잘 수행하는지 확인하고자 함
- 실험 설정은 이전 설정에서 가장 잘 되었던 1024, 2048, sigma 1.0으로 수행
- 결과
    - 0.01 정도의 매우 저조한 분류율 보임.
    - 단순 분류 task였음에도 이런 결과가 나왔다는 건 즉 모델에 화자를 분류할 능력이 없음을 의미


추후
!여타 prototypical classification 연구 탐색 (Gram matrix)
BERT 대신 RoBERTa 도입. 추후에는 T-5까지 고려 (X) 이미 RoBERTa 성능으로 충분함
손실에서 앵커 거리의 중요도를 훈련이 진행됨에 따라 천천히 올리는 기법
!분류 태스크로 멀티태스크 러닝(MTL)이나 지식 증류를 도입하여 성능을 올리는 것을 시도
여러 개의 인코더를 두어 여러 관점을 합쳐 task를 수행하도록 하는 것도 방법이다. 
!LLM은 데이터만 주면 알아서 작동하는 만능 로봇이 아니다. 모델이 입력을 어떻게 이해하도록 할 것인지 잘 구성해야 한다.
언급부 정보를 써먹는 것이 가장 이상적. 다만 이 경우 모델이 언급부를 자동으로 식별하도록 all-in-one인 방향으로 가고 싶다.
당장은 모델이 narrative-비narrative 이진 분류 성능이 굉장히 높았던 점에서 착안하여 연속 화자 이진 분류로 모델 성능을 높일 수 없을지 보고 싶다.


domain incremental 고려 (여러 권의 책으로 batch 구성)
class imbalance 문제 고려

tensorboard --logdir="/workspace/models_storage/cluster/mse_loss/log" --bind_all

service ssh start