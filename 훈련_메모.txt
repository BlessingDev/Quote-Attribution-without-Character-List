
다음 실험에서는 anchor 수를 줄이고, lr을 올려보자

HDBSCAN 대신 Affinity Propagation을 해보자(나중)

정말 anchor 수와 memory step이 성능에 영향을 주는지 확인해 볼 필요가 있을 것 같다.

군집이 너무 안 만들어지는 경향이 있다.

loss에 대한 분석도 필요하다.
분석 완료. neg_spcl의 영향을 줄일 필요가 있음.

우리 task는 발화자를 식별하는 것이기에 굳이 형태도 다르고 담는 내용도 천차만별일 narrative를 하나의 군집으로 모이게 할 필요는 없을 것이다.
이들이 오히려 훈련에 지대한 악영향을 끼치고 있을 수 있다.

세팅 변경 이력
0529_1724부터
- log(l+1) 로스 적용

0530_1245
- 마지막 선형 변환 후에 tanh 변환 적용
- 의미 없음. 오히려 약간 떨어짐.

0530_1814
- tanh층 제거
- mem step 늘림
- 확실히 더 일반화되는 느낌 있음. v1과 loss 사이의 consistency도 증가.

0531_0955
- anchor 저장 무한하게
- 별로 다른 모습을 보여주지 못함

0531_1341
- narrative를 군집화에서 제외(positive loss 계산 안 함)
- loss에 beta 도입(기본값 0.5 사용)
- loss가 낮은 수치로 나오긴 했으나 V1이 거의 그대로 나옴. narrative를 안 세어서 얻는 메리트 없는 것으로 보임. 

0531_1637
- beta에서 centroid에 더 가중치를 두도록 한다.
- anchor 계산시 narrative는 제외
- 변경한 설정의 영향이 너무 작았다. lr을 5배 올렸다.
- NAN weight 발생

0601_1119
- anchor 무한 상태에서 mem step 늘려보기
- beta에서 centroid에 더 가중치를 두도록 한다.
- 별로 효과 없음

0603_1017
- loss 재설계
    - 편차가 커지면 phi가 작아져 해당 centroid를 더 고려하도록 phi 계산식을 수정
    - 거리가 가까운 negative point, 먼 positive point를 더 고려하도록 한다. 단순 평군으로는 거리가 먼 negative와 가까운 negative가 있으면 평균은 그 사이 값으로 적절하게 나올 것이다. 다수의 포인트를 가까운 곳에 모아야 하는 문제 특성상 이는 적절하지 않다.
    - 가까운 걸 멀리 보내려면 exp, 먼 걸 가까이 가져오려면 log. 현재는 활성 함수가 무조건 exp이니 가까운 걸 멀리 보내는 데만 유리하다.
    - 확률 개념이 아니니 temperature를 적용해야 할 직관적인 이유가 없다. phi만 적용.
- numerical 불안정 문제 발생
- 성능은 다소 향상되는 것으로 확인

0603_1326
- 수치 불안정성 문제 해결을 위해 weight decay, gradient clipping 추가

0603_1820
- loss에 alpha beta 추가
- 성능이 상당히 향상되는 모습을 보여줬지만, weight decay 없이는 수치 불안정성 발생

0604_1058
- max_grad_norm 축소
- MLP에 은닉층 추가
- weight decay 0.1
- 별로 인상적이지 않은 성능

0604_1335
- MLP 은닉층 한 개 제거
- loss를 gradient 관점에서 재설계
    - negative에 log를 적용하여 가까이 있는 점을 빠르게 밀어낸다.
    - postive에 exp를 적용하여 멀리 있는 점을 빠르게 끌고온다.
- loss_sig를 centroid를 중시하도록 설정
- 별로 효과적이지 않았음
- 후속 조치로 실제 거리가 어느 정도 선에서 나오는지 확인해볼 계획

0604_1817
- loss를 재설계
    - negative set을 log에서 선형으로 변경. 일정 이상 멀어지면 gradient가 급격히 소멸하는 부분이 문제라고 생각
    - alpha를 1로 변경, beta 0.8
- 그다지 인상적이지 않은 성능

0605_1111
- loss를 재설계
    - negative set을 선형에서 n*log(n+e) 형태로 변형. 초반에 빠르게 멀어지고 그 이후에도 어느정도 gradient를 유지하게 될 것
- 한 지점에 모든 점이 수렴하지 않고, negative 간에 멀어지면서 점점 퍼지는 모습을 확인함
- loss가 일정 이하로 내려가지 못하고 횡보하는 양상 보임. 언더피팅
- 다음 실험에서는 모델 복잡도를 높여서 시도

0605_1652
- 모델 복잡도 올림. 
    - MLP layer 추가. 
    - Add&Norm 층 추가.
- loss 재설계
    - loss를 살펴보았을 때 batch내 pos간 거리는 비교적 짧게 나오고, centroid 거리는 비교적 멀게 나온다.
    - batch내 negative간 거리는 짧게 나오고 centroid와의 거리가 길게 나온다.
    - 따라서 sig 가중치를 교차해서 적용하도록 재설계했다.
    - pos에서는 centroid와의 거리를 더 고려해야 하고, negative에서는 batch내 샘플 간 거리를 더 고려해야 한다.
- phi 계산식 수정
- homogenity가 유래 없이 증가하는 양상을 확인. 이는 노린 바에 부합
- 수치 불안정성이 부각됨. postive 거리가 멀어질수록 어느 순간 exp 함수가 inf로 발산할 위험이 생김.

0605_1800
- loss 재설계
    - 마이너 변경
    - alpha를 더 작은 수로 변경. beta도 비슷한 비율로 변경. (0.15, 0.2)
    - exp은 발산하는 함수이기 때문에 거리가 먼 점을 될 수 있으면 가까이 끌어와야 함
    - homogeniety를 높인다는 성과를 달성했으므로 컨셉은 그대로 가져감
- 상당히 높은 성능을 보임
- 그러나 수치 불안정성이 너무 눈에 띔

0606_1747
- loss 값을 모두 더하여 나중에 log를 취하는 것이 아니라 각 샘플마다 log를 구해서 더하도록 코딩 변경
- 실제 로스 값 계산을 확인해보니 좌표 수치가 16~-16까지 벌어지는 것을 확인.
    - normalize한 거리도 수백 수준까지 증가, 그 결과 loss가 inf로 발산한 듯함.
    - 너무 높은 자유도. 이를 해결하기 위해 최종 층에 활성 함수로 tanh를 추가함
- 결과적으로 loss가 0으로 수렴했지만, 군집화 성능은 좋지 않았다.
    - 이는 loss 구조의 태생적인 문제로부터 기인한 것으로 보인다. 
    - pos만 0으로 만들면 negative가 얼마나 되는지와 관계 없이 loss가 0이 된다. 이는 clustering의 취지와도 어긋난다.

0606_2118
- 모델 저장 기준을 v1 점수로 변경
- mu_p, mu_neg을 0.5, 0.2로 변경, negative를 더 강하게 밀어내도록 했다.
- pos / neg 계산을 (pos + 1) / neg로 변경하여 pos를 0으로 만든 후에도 neg를 계속 밀어내도록 하였다.
- tanh를 적용한 상태에서 이전과 유사한 성능을 달성. 그러나 0605_1800에는 못 미치는 성능 보임
- loss 최적화가 잘 되지 않는 모습을 보이며 언더피팅

0607_0931
- 언더피팅의 이유가 자유도 부족에 있다고 가정. decoder 차원을 2048에서 5000으로 늘림
- 이전에 잘 되었던 mu_p와 mu_n인 0.15와 0.2와 비슷하게 0.5, 0.6으로 재설정
- 성능 잘 안 나옴. 오히려 0606_2118보다 못한 모습 보임
- 자유도를 늘렸음에도 언더피팅. loss 최적화가 왜 안 되는지 이유 분석 필요

0607_1535
- 멀리 있는 postive를 빠르게 끌어오도록 mu_p를 크게, 가까이 있는 negative를 더 열심히 밀어내도록 mu_n을 작게 설정 (0.8, 0.3)
- loss가 줄어들지 않는 현상을 고치기 위해 lr 증가
- 최적화를 위해 단일 책 최적화를 n회씩 진행하는 기법 (n에폭씩 묶어서 에폭을 진행)
- 성능 여전히 안 좋음. 최적화 안 됨.

0607_1754
- lr 더 증가
- mu_p와 mu_n을 이전 가장 좋았던 값인 0.15와 0.2로 변경
    - tanh에 자유도를 추가. 5로 설정
- 성능 너무 나쁨

0608_1214
- mse_loss의 reduction을 기존 sum에서 mean으로 변경.
    - 그에 따라 수의 규모가 줄어드니 mu_p를 1.0으로 변경
    - mu_n도 0.8로 변경
    - tanh 활성 제거
- 성능이 매우 나쁨
- 단순 성능 비교를 위해 여기서부터 파라미터를 하나씩 바꾸며 실험 진행

0608_2203
- 위 설정에서 mu_p가 미치는 영향을 파악하기 위해 mu_p만 0.5로 줄임
- 안 좋은 성능


0610_1138
- 위 설정에서 mu_n이 미치는 영향을 파악하기 위해 mu_p를 1.0으로 mu_n을 0.5로 설정
- tanh 활성 추가
- 첫 에폭에서 발화자 비율 파이 차트 그리기
- 매 반복마다 평균 pos 거리, 평균 neg 거리 기록. 도표 그리기.
- 군집화 평가를 Fowlkes-Mallows Score로 진행

0612_

추후
여타 prototypical classification 연구 탐색 (Gram matrix)
GRU 도입하여 순차적 정보 고려

domain incremental 고려 (여러 권의 책으로 batch 구성)
class imbalance 문제 고려

tensorboard --logdir="/workspace/models_storage/cluster/mse_loss/log" --bind_all

service ssh start